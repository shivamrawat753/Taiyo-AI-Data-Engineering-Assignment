# -*- coding: utf-8 -*-
"""Web_Scrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16u7dOdgf4tFQW4aoYY3QPBCM4Fx5ntvF
"""

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

class WebScraper:
    def __init__(self):
        # Initialize the Chrome WebDriver
        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.headless = True
        self.driver = webdriver.Chrome(options=chrome_options)

        # URL of the website you want to scrape
        self.base_url = 'https://etenders.gov.in/eprocure/app'
        self.data_list = []

    def scrape_tender_data(self):
        try:
            # Navigate to the initial page
            self.driver.get(self.base_url)
            time.sleep(2)

            # Find the table with id "activeTenders"
            table_element = self.driver.find_element(By.ID, "activeTenders")

            # Find all the rows (tr elements) inside the table
            rows = table_element.find_elements(By.TAG_NAME, "tr")[0:]

            for row in rows:
                link_element = row.find_element(By.TAG_NAME, "a")
                link = link_element.get_attribute("href")
                link_text = link_element.text  # Get the link text before clicking

                self.driver.execute_script("window.open('', '_blank');")
                self.driver.switch_to.window(self.driver.window_handles[-1])
                self.driver.get(link)
                time.sleep(2)

                entry_data = self.extract_data_from_page()
                entry_data['url'] = link

                self.data_list.append(entry_data)

                self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
                time.sleep(2)

        except Exception as e:
            print(f"Error: {e}")
            self.driver.quit()

    def extract_data_from_page(self):
        data = {}
        try:
            data['Tender ID'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Tender ID")]/following-sibling::td/b').text
            data['Tender Title'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Title")]/following-sibling::td/b').text
            data['Reference No.'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Tender Reference Number")]/following-sibling::td/b').text
            data['Organisation Chain'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Organisation Chain")]/following-sibling::td/b').text
            data['Location'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Location")]/following-sibling::td').text
            data['E-Published Date'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Published Date")]/following-sibling::td').text
            data['Bid Submission Closing Date'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Submission End Date")]/following-sibling::td').text
            data['Tender Opening Date'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Bid Opening Date")]/following-sibling::td').text
            data['Tender Value in Rupees'] = self.driver.find_element(By.XPATH, '//td[contains(b, "Tender Value in â‚¹")]/following-sibling::td/b').text
        except Exception as e:
            print(f"Error extracting data: {e}")

        return data

    def save_to_csv(self):
        # Close the WebDriver when done
        self.driver.quit()

        # Convert the list of dictionaries into a Pandas DataFrame
        df = pd.DataFrame(self.data_list)

        # Save the DataFrame to a CSV file
        df.to_csv('tender_data.csv', index=False)

if __name__ == "__main__":
    scraper = WebScraper()
    scraper.scrape_tender_data()
    scraper.save_to_csv()

# Display the DataFrame
pd.read_csv('tender_data.csv')